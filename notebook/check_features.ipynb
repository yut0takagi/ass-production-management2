{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e491d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import itertools\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tools.get_describe_record import get_describe_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f9ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_excel(\"../data/raw/kadai.xlsx\")\n",
    "raw_df.head(2)\n",
    "processe = raw_df.copy()\n",
    "processe[\"batch_id\"] = processe.groupby(\"process_end_time\").ngroup()\n",
    "processe.head()\n",
    "tagged_anormaly = pd.read_csv(\"../data/processed/processe_tagged_anormaly.csv\")\n",
    "tagged_anormaly.head()\n",
    "anomaly = tagged_anormaly[tagged_anormaly[\"is_anomaly\"] == 1]\n",
    "nomaly = tagged_anormaly[tagged_anormaly[\"is_anomaly\"] == 0]\n",
    "## 異常スパイク部分の予測モデル構築\n",
    "train_df = anomaly[anomaly[\"batch_id\"] < 75]\n",
    "test_df = anomaly[anomaly[\"batch_id\"] >= 75]\n",
    "train_df = train_df.drop(columns=[\"is_anomaly\",\"batch_OV_std\", \"batch_id\", \"process_end_time\", \"final_mes_time\"])\n",
    "test_df = test_df.drop(columns=[\"is_anomaly\",\"batch_OV_std\", \"batch_id\", \"process_end_time\", \"final_mes_time\"])\n",
    "\n",
    "#Attention:------------------Attention:------------------Attention:------------------Attention:------------------Attention:------------------  \n",
    "SENSOR_COLS = [f\"X{i}\" for i in [27, 30, 41, 25, 36]]\n",
    "LAGS = [1, 2, 3, 5, 10, 20]\n",
    "WINS = [3, 5, 10, 20]   # rolling window\n",
    "#Attention:------------------Attention:------------------Attention:------------------Attention:------------------Attention:------------------  \n",
    "\n",
    "def feature_engineering1(df, SENSOR_COLS=SENSOR_COLS, LAGS=LAGS, WINS=WINS):\n",
    "    base = df[SENSOR_COLS].copy()\n",
    "    feats = {}  # ★ここに全部貯める\n",
    "\n",
    "    for c in SENSOR_COLS:\n",
    "        s = df[c]\n",
    "\n",
    "        # lag\n",
    "        for l in LAGS:\n",
    "            feats[f\"{c}_lag{l}\"] = s.shift(l)\n",
    "\n",
    "        # diff / pct\n",
    "        feats[f\"{c}_diff1\"] = s.diff(1)\n",
    "        feats[f\"{c}_diff2\"] = s.diff(2)\n",
    "        feats[f\"{c}_pct1\"]  = s.pct_change(1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # rolling\n",
    "        for w in WINS:\n",
    "            r = s.rolling(w, min_periods=1)\n",
    "            rmean = r.mean()\n",
    "            rstd  = r.std()\n",
    "            feats[f\"{c}_rmean{w}\"] = rmean\n",
    "            feats[f\"{c}_rstd{w}\"]  = rstd\n",
    "            feats[f\"{c}_rmax{w}\"]  = r.max()\n",
    "            feats[f\"{c}_rmin{w}\"]  = r.min()\n",
    "            feats[f\"{c}_z{w}\"]     = (s - rmean) / (rstd + 1e-9)\n",
    "            feats[f\"{c}_dev{w}\"]   = s - rmean\n",
    "        \n",
    "        feats[f\"{c}_energy\"] = (\n",
    "            feats[f\"{c}_rstd20\"] * feats[f\"{c}_rmax20\"]\n",
    "        )\n",
    "\n",
    "        feats[f\"{c}_jump\"] = s - s.shift(5)\n",
    "\n",
    "    feat_df = pd.concat([base, pd.DataFrame(feats, index=df.index)], axis=1)\n",
    "    feat_df = feat_df.ffill().bfill().fillna(0)\n",
    "    return feat_df\n",
    "\n",
    "def struct_model_spike(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    SENSOR_COLS = [f\"X{i}\" for i in [27, 30, 41, 25, 36]],\n",
    "    LAGS = [1, 2, 3, 5, 10, 20],\n",
    "    WINS = [3, 5, 10, 20]\n",
    "):\n",
    "    train_feature_df = feature_engineering1(train_df, SENSOR_COLS=SENSOR_COLS, LAGS=LAGS, WINS=WINS)\n",
    "    test_feature_df = feature_engineering1(test_df, SENSOR_COLS=SENSOR_COLS, LAGS=LAGS, WINS=WINS)\n",
    "    #train_feature_df.head()\n",
    "\n",
    "    # spike用特徴量\n",
    "    X_sp = train_feature_df\n",
    "    y_sp = np.log1p(train_df[\"OV\"].values)\n",
    "\n",
    "    # 高OVを重くする（まずは80%点を基準）\n",
    "    q = np.percentile(train_df[\"OV\"].values, 90)  # “頂点”の境界\n",
    "    w = np.ones_like(train_df[\"OV\"].values, dtype=float)\n",
    "    w[train_df[\"OV\"].values >= q] = 1 + ((train_df[\"OV\"].values[train_df[\"OV\"].values >= q] / (q+1e-9))**3)\n",
    "    w = np.clip(w, 1.0, np.percentile(w, 95))\n",
    "\n",
    "    spike_predict_model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    spike_predict_model.fit(X_sp, y_sp, sample_weight=w)\n",
    "\n",
    "    with open(\"../data/model/spike_predict_rf.pkl\", \"wb\") as f:\n",
    "        pickle.dump(spike_predict_model, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf881619",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 平常時の予測\n",
    "feat =[\"process_end_time\", \"final_mes_time\", \"OV\"]+  [f\"X{i}\" for i in range(1, 84)]\n",
    "\n",
    "train_df = nomaly.loc[nomaly[\"batch_id\"] < 75, feat]\n",
    "test_df  = nomaly.loc[nomaly[\"batch_id\"] >= 75, feat]\n",
    "\n",
    "\n",
    "normal_features = [\"X14\", \"X30\", \"X33\", \"X83\"] + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\", \"OV\"]\n",
    "\n",
    "\n",
    "def df_set_datetime(df, col_name):\n",
    "    for col in col_name:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "# process_end_timeを用いて経過時間, ラグを取得する。\n",
    "def get_elapsed_day(df, base_time=None):\n",
    "    if base_time == None:\n",
    "        base_time = df['process_end_time'].min()\n",
    "    df['elapsed_day'] = (df['process_end_time'] - base_time).dt.days\n",
    "    return df\n",
    "\n",
    "def set_LagOV(df,target=\"OV\", lag_record_num=1, window=3):\n",
    "    df[f\"{target}_lag{lag_record_num}\"] = df[target].shift(lag_record_num)\n",
    "    df[f\"{target}_diff\"] = df[target].diff(1).shift(1)  \n",
    "    df[f\"{target}_roll_mean{window}\"] = df[target].rolling(window).mean().shift(1)\n",
    "    df[f\"{target}_roll_std{window}\"] = df[target].rolling(window).std().shift(1)\n",
    "    #df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def feature_engineering2(df, normal_features):\n",
    "    df = df.copy()\n",
    "    df = df_set_datetime(df, [\"process_end_time\", \"final_mes_time\"])\n",
    "    df = get_elapsed_day(df)\n",
    "    df = set_LagOV(df)\n",
    "    df = df.drop(columns=[col for col in df.columns if col not in normal_features])\n",
    "    return df\n",
    "\n",
    "def struct_model_flat(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    normal_features,\n",
    "):\n",
    "    train_df = feature_engineering2(train_df, normal_features)\n",
    "    test_df = feature_engineering2(test_df, normal_features)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        #(\"scaler\", StandardScaler()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            metric=\"rmse\",\n",
    "            verbose=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    train_X = train_df[normal_features].drop(columns=[\"OV\"])\n",
    "    pipeline.fit(train_X, train_df[\"OV\"])\n",
    "\n",
    "    with open(\"../data/model/non_spike_predict.pkl\", \"wb\") as f:\n",
    "        pickle.dump(pipeline, f)\n",
    "    #test_X = test_df[normal_features].drop(columns=[\"OV\"])\n",
    "    #pred = pipeline.predict(test_X)\n",
    "    #rmse = np.sqrt(mean_squared_error(test_df[\"OV\"], pred))\n",
    "    #print(f\"RMSE (nomaly only): {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea116169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# スパイク発生ロット\n",
    "SENSOR_COLS = [f\"X{i}\" for i in [27, 30, 41, 25, 36]]\n",
    "LAGS = [1, 2, 3, 5, 10, 20]\n",
    "WINS = [3, 5, 10, 20]   # rolling window\n",
    "\n",
    "# 通常ロット\n",
    "normal_features = [\"X14\", \"X30\", \"X33\", \"X83\"] + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\"]\n",
    "rows = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f495aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_all(\n",
    "    processe,\n",
    "    SENSOR_COLS = [f\"X{i}\" for i in [27, 30, 41, 25, 36]],\n",
    "    LAGS = [1, 2, 3, 5, 10, 20],\n",
    "    WINS = [3, 5, 10, 20],\n",
    "    normal_features = [\"X14\", \"X30\", \"X33\", \"X83\"] + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\"]\n",
    "):\n",
    "    # 全体予測\n",
    "    with open(\"../data/model/spike_detection_IFmodel.pkl\", \"rb\") as f:\n",
    "        spike_model = pickle.load(f)\n",
    "\n",
    "    with open(\"../data/model/spike_predict_rf.pkl\", \"rb\") as f:\n",
    "        spike_predict_model = pickle.load(f)\n",
    "\n",
    "    with open(\"../data/model/non_spike_predict.pkl\", \"rb\") as f:\n",
    "        non_spike_predict_model = pickle.load(f)\n",
    "\n",
    "    with open(\"../data/score/IF_train_score.pkl\", \"rb\") as f:\n",
    "        score_train = pickle.load(f)\n",
    "    thr = np.quantile(score_train, 0.90)\n",
    "\n",
    "    for batch in range(75, 100):\n",
    "\n",
    "        batch_df = processe[processe[\"batch_id\"] == batch]\n",
    "        batch_OV = batch_df[\"OV\"].values\n",
    "\n",
    "        # --- スパイク判定 ---\n",
    "        desc_df = get_describe_record(df=processe, batch=batch)\n",
    "        if hasattr(desc_df, \"ndim\") and desc_df.ndim == 1:\n",
    "            desc_df = desc_df.to_frame().T\n",
    "\n",
    "        score_test = -spike_model.score_samples(desc_df)\n",
    "        is_spike = int((score_test >= thr).item())\n",
    "\n",
    "        # --- 予測 ---\n",
    "        if is_spike == 1:\n",
    "            feature_df = feature_engineering1(\n",
    "                batch_df,\n",
    "                SENSOR_COLS=SENSOR_COLS,\n",
    "                LAGS=LAGS,\n",
    "                WINS=WINS\n",
    "            )\n",
    "            pred = spike_predict_model.predict(feature_df)\n",
    "            pred = np.expm1(pred)\n",
    "        else:\n",
    "            feature_df = feature_engineering2(\n",
    "                batch_df,\n",
    "                normal_features\n",
    "            )\n",
    "            pred = non_spike_predict_model.predict(feature_df)\n",
    "\n",
    "        # --- val_df に追加（★ここが重要） ---\n",
    "        tmp_df = pd.DataFrame({\n",
    "            \"trueOV\": batch_OV,\n",
    "            \"predOV\": pred,\n",
    "            \"batch_id\": batch,\n",
    "            \"is_spike\": is_spike\n",
    "        })\n",
    "        rows.append(tmp_df)\n",
    "\n",
    "    val_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    rmse_all = np.sqrt(mean_squared_error(val_df[\"trueOV\"], val_df[\"predOV\"]))\n",
    "    rmse_sp  = np.sqrt(mean_squared_error(val_df.loc[val_df[\"is_spike\"]==1,\"trueOV\"],\n",
    "                                        val_df.loc[val_df[\"is_spike\"]==1,\"predOV\"]))\n",
    "    rmse_no  = np.sqrt(mean_squared_error(val_df.loc[val_df[\"is_spike\"]==0,\"trueOV\"],\n",
    "                                        val_df.loc[val_df[\"is_spike\"]==0,\"predOV\"]))\n",
    "\n",
    "    #print(f\"RMSE all   : {rmse_all:.3f}\")\n",
    "    #print(f\"RMSE spike : {rmse_sp:.3f}\")\n",
    "    #print(f\"RMSE normal: {rmse_no:.3f}\")\n",
    "    \n",
    "    \"\"\"plt.figure(figsize=(8, 5))\n",
    "    mask_spike = val_df[\"is_spike\"] == 1\n",
    "    plt.plot(val_df.index[mask_spike], val_df[\"trueOV\"][mask_spike], label=\"True OV(spike)\", alpha=0.3)\n",
    "    mask_normal = val_df[\"is_spike\"] == 0\n",
    "    plt.plot(val_df.index[mask_normal], val_df[\"trueOV\"][mask_normal], label=\"True OV(normal)\", alpha=0.3)\n",
    "    plt.plot(val_df.index, val_df[\"predOV\"], label=\"Predicted OV\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    return rmse_all, rmse_sp, rmse_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41200c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1837620 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "raw_df = pd.read_excel(\"../data/raw/kadai.xlsx\")\n",
    "raw_df.head(2)\n",
    "processe = raw_df.copy()\n",
    "processe[\"batch_id\"] = processe.groupby(\"process_end_time\").ngroup()\n",
    "processe.head()\n",
    "tagged_anormaly = pd.read_csv(\"../data/processed/processe_tagged_anormaly.csv\")\n",
    "\n",
    "features = [f\"X{i}\" for i in range(1, 84)]\n",
    "comb_4A = list(itertools.combinations(features, 4))\n",
    "comb_4B = list(itertools.combinations(features, 4))\n",
    "best_rmse = 100\n",
    "\n",
    "for combA in tqdm(comb_4A):\n",
    "    for combB in comb_4B:\n",
    "        try:\n",
    "            anomaly = tagged_anormaly[tagged_anormaly[\"is_anomaly\"] == 1]\n",
    "            nomaly = tagged_anormaly[tagged_anormaly[\"is_anomaly\"] == 0]\n",
    "            ## 異常スパイク部分の予測モデル構築\n",
    "            train_df = anomaly[anomaly[\"batch_id\"] < 75]\n",
    "            test_df = anomaly[anomaly[\"batch_id\"] >= 75]\n",
    "            train_df = train_df.drop(columns=[\"is_anomaly\",\"batch_OV_std\", \"batch_id\", \"process_end_time\", \"final_mes_time\"])\n",
    "            test_df = test_df.drop(columns=[\"is_anomaly\",\"batch_OV_std\", \"batch_id\", \"process_end_time\", \"final_mes_time\"])\n",
    "\n",
    "            train_df = processe[processe[\"batch_id\"] < 75]\n",
    "            test_df = processe[processe[\"batch_id\"] >= 75]\n",
    "            struct_model_spike(\n",
    "                train_df,\n",
    "                test_df,\n",
    "                SENSOR_COLS = list(combA),\n",
    "                LAGS = [1, 2, 3, 5, 10, 20],\n",
    "                WINS = [3, 5, 10, 20]\n",
    "            )\n",
    "            \n",
    "            feat =[\"process_end_time\", \"final_mes_time\", \"OV\"]+  list(combB)\n",
    "            train_df = nomaly.loc[nomaly[\"batch_id\"] < 75, feat]\n",
    "            test_df  = nomaly.loc[nomaly[\"batch_id\"] >= 75, feat]\n",
    "            normal_features = list(combB) + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\", \"OV\"]\n",
    "\n",
    "            struct_model_flat(\n",
    "                train_df,\n",
    "                test_df,\n",
    "                normal_features = list(combB) + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\", \"OV\"]\n",
    "            )\n",
    "            rmse_all, rmse_sp, rmse_no = pred_all(\n",
    "                processe,\n",
    "                SENSOR_COLS = list(combA),\n",
    "                LAGS = [1, 2, 3, 5, 10, 20],\n",
    "                WINS = [3, 5, 10, 20],\n",
    "                normal_features = list(combB) + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\"]\n",
    "            )\n",
    "            print(\n",
    "                f\"combA: {combA}\",\n",
    "                f\"combB: {combB}\",\n",
    "                f\"RMSE all   : {rmse_all:.3f}\",\n",
    "                f\"RMSE spike : {rmse_sp:.3f}\",\n",
    "                f\"RMSE normal: {rmse_no:.3f}\",\n",
    "                sep=\"\\t\"\n",
    "            )\n",
    "            if best_rmse > rmse_all:\n",
    "                best_rmse = rmse_all\n",
    "                best_comb = (comb_4A, comb_4B, rmse_all, rmse_sp, rmse_no)\n",
    "                with open(\"best_comb_temp.pkl\", \"wb\") as f:\n",
    "                    pickle.dump(best_comb, f)\n",
    "\n",
    "            if rmse_no > 25:\n",
    "                comb_4B.remove(combB)\n",
    "                break\n",
    "        except:\n",
    "            pass\n",
    "        if rmse_sp > 40:\n",
    "            comb_4A.remove(combA)\n",
    "            break\n",
    "    if rmse_all < 40:\n",
    "        best_comb = (comb_4A, comb_4B, rmse_all, rmse_sp, rmse_no)\n",
    "        with open(\"best_comb.pkl\", \"wb\") as f:\n",
    "            pickle.dump(best_comb, f)\n",
    "        break\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31104059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
