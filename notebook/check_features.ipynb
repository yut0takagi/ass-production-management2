{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0e491d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import itertools\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from tools.get_describe_record import get_describe_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfa4e420",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "featuresA = [f\"X{i}\" for i in range(1, 84)]\n",
    "featuresB = [f\"X{i}\" for i in range(1, 84)]\n",
    "\n",
    "#featuresA = [\"X30\", \"X41\", \"X28\", \"X33\", \"X76\", \"X31\"]\n",
    "#featuresB = [\"X41\", \"X36\", \"X27\", \"X29\", \"X35\", \"X26\"]\n",
    "\n",
    "comb_A = list(itertools.combinations(featuresA, 4))\n",
    "comb_B = list(itertools.combinations(featuresB, 4))\n",
    "\n",
    "comb_AB = list(itertools.product(comb_A, comb_B))\n",
    "random.shuffle(comb_AB)\n",
    "\n",
    "best_rmse = 100\n",
    "rmse_sp = 50\n",
    "rmse_no = 100\n",
    "rmse_all = 100\n",
    "best_comb = None\n",
    "\n",
    "# 異常スパイク検出用\n",
    "batch_describe_df = pd.read_csv(\"../data/processed/batch_describe_df.csv\")\n",
    "test_batch = batch_describe_df.iloc[75:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ccc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 異常スパイク検知モデルと閾値スコアの読み込み\n",
    "with open(\"../data/model/spike_detection_IFmodel.pkl\", \"rb\") as f:\n",
    "    spike_detection_model = pickle.load(f)\n",
    "with open(\"../data/score/IF_train_score.pkl\", \"rb\") as f:\n",
    "    spike_detection_score = pickle.load(f)\n",
    "thr = np.quantile(spike_detection_score, 0.90)\n",
    "\n",
    "score_test  = -spike_detection_model.score_samples(test_batch)\n",
    "pred_iso = (score_test >= thr).astype(int)\n",
    "spike_batches = test_batch.index[pred_iso == 1].tolist()\n",
    "normal_batches = test_batch.index[pred_iso == 0].tolist()\n",
    "print(f\"spike_batches: {spike_batches}\")\n",
    "print(f\"normal_batches: {normal_batches}\")\n",
    "\n",
    "\n",
    "# 学習用データ\n",
    "tagged_data = pd.read_csv(\"../data/processed/processe_tagged_anormaly.csv\")\n",
    "anomaly_df = tagged_data[tagged_data[\"is_anomaly\"] == 1]\n",
    "normal_df = tagged_data[tagged_data[\"is_anomaly\"] == 0]\n",
    "anomaly_train_df = anomaly_df.iloc[:75]\n",
    "nomaly_train_df = normal_df.iloc[:75]\n",
    "\n",
    "# テスト用データ\n",
    "test_df = pd.read_csv(\"../data/processed/processe_tagged_anormaly.csv\")\n",
    "anomaly_test_df = test_df[test_df[\"batch_id\"].isin(spike_batches)]\n",
    "normaly_test_df  = test_df[test_df[\"batch_id\"].isin(normal_batches)]\n",
    "test_df_original = test_df[test_df[\"batch_id\"] >=75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d130eecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴量エンジニアリング\n",
    "def feature_engineering1(df, SENSOR_COLS, LAGS, WINS):\n",
    "    base = df[SENSOR_COLS].copy()\n",
    "    feats = {}  # ★ここに全部貯める\n",
    "\n",
    "    for c in SENSOR_COLS:\n",
    "        s = df[c]\n",
    "\n",
    "        # lag\n",
    "        for l in LAGS:\n",
    "            feats[f\"{c}_lag{l}\"] = s.shift(l)\n",
    "\n",
    "        # diff / pct\n",
    "        feats[f\"{c}_diff1\"] = s.diff(1)\n",
    "        feats[f\"{c}_diff2\"] = s.diff(2)\n",
    "        feats[f\"{c}_pct1\"]  = s.pct_change(1).replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "        # rolling\n",
    "        for w in WINS:\n",
    "            r = s.rolling(w, min_periods=1)\n",
    "            rmean = r.mean()\n",
    "            rstd  = r.std()\n",
    "            feats[f\"{c}_rmean{w}\"] = rmean\n",
    "            feats[f\"{c}_rstd{w}\"]  = rstd\n",
    "            feats[f\"{c}_rmax{w}\"]  = r.max()\n",
    "            feats[f\"{c}_rmin{w}\"]  = r.min()\n",
    "            feats[f\"{c}_z{w}\"]     = (s - rmean) / (rstd + 1e-9)\n",
    "            feats[f\"{c}_dev{w}\"]   = s - rmean\n",
    "        \n",
    "        feats[f\"{c}_energy\"] = (\n",
    "            feats[f\"{c}_rstd20\"] * feats[f\"{c}_rmax20\"]\n",
    "        )\n",
    "\n",
    "        feats[f\"{c}_jump\"] = s - s.shift(5)\n",
    "\n",
    "    feat_df = pd.concat([base, pd.DataFrame(feats, index=df.index)], axis=1)\n",
    "    feat_df = feat_df.ffill().bfill().fillna(0)\n",
    "    return feat_df\n",
    "\n",
    "def df_set_datetime(df, col_name):\n",
    "    for col in col_name:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "    return df\n",
    "\n",
    "# process_end_timeを用いて経過時間, ラグを取得する。\n",
    "def get_elapsed_day(df, base_time=None):\n",
    "    if base_time == None:\n",
    "        base_time = df['process_end_time'].min()\n",
    "    df['elapsed_day'] = (df['process_end_time'] - base_time).dt.days\n",
    "    return df\n",
    "\n",
    "def set_LagOV(df,target=\"OV\", lag_record_num=1, window=3):\n",
    "    df[f\"{target}_lag{lag_record_num}\"] = df[target].shift(lag_record_num)\n",
    "    df[f\"{target}_diff\"] = df[target].diff(1).shift(1)  \n",
    "    df[f\"{target}_roll_mean{window}\"] = df[target].rolling(window).mean().shift(1)\n",
    "    df[f\"{target}_roll_std{window}\"] = df[target].rolling(window).std().shift(1)\n",
    "    #df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def feature_engineering2(df, normal_features):\n",
    "    df = df.copy()\n",
    "    df = df_set_datetime(df, [\"process_end_time\", \"final_mes_time\"])\n",
    "    df = get_elapsed_day(df)\n",
    "    df = set_LagOV(df)\n",
    "    df = df.drop(columns=[col for col in df.columns if col not in normal_features])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d41200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (comb_A, comb_B) in comb_AB:\n",
    "    # スパイクモデルの学習\n",
    "    SENSOR_COLS = list(comb_A)\n",
    "    LAGS = [1, 2, 3, 5, 10, 20]\n",
    "    WINS = [3, 5, 10, 20]\n",
    "\n",
    "    train_feature_df = feature_engineering1(anomaly_train_df, SENSOR_COLS=SENSOR_COLS, LAGS=LAGS, WINS=WINS)\n",
    "\n",
    "    # モデルの学習\n",
    "    X_sp = train_feature_df\n",
    "    y_sp = np.log1p(anomaly_train_df[\"OV\"].values)\n",
    "\n",
    "    # 高OVを重くする（まずは80%点を基準）\n",
    "    q = np.percentile(anomaly_train_df[\"OV\"].values, 90)  # “頂点”の境界\n",
    "    w = np.ones_like(anomaly_train_df[\"OV\"].values, dtype=float)\n",
    "    w[anomaly_train_df[\"OV\"].values >= q] = 1 + ((anomaly_train_df[\"OV\"].values[anomaly_train_df[\"OV\"].values >= q] / (q+1e-9))**3)\n",
    "    w = np.clip(w, 1.0, np.percentile(w, 95))\n",
    "\n",
    "    spike_predict_model = RandomForestRegressor(\n",
    "        n_estimators=600,\n",
    "        max_depth=18,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    spike_predict_model.fit(X_sp, y_sp, sample_weight=w)\n",
    "\n",
    "    test_feature_df = feature_engineering1(anomaly_test_df, SENSOR_COLS=SENSOR_COLS, LAGS=LAGS, WINS=WINS)\n",
    "    spiked_pred = spike_predict_model.predict(test_feature_df)\n",
    "    spiked_pred = np.expm1(spiked_pred)\n",
    "    spiked_pred_series = pd.Series(spiked_pred, index=anomaly_test_df.index)\n",
    "    rmse_sp = np.sqrt(mean_squared_error(anomaly_test_df[\"OV\"], spiked_pred))\n",
    "    #print(spiked_pred)\n",
    "\n",
    "    # フラット部分の学習\n",
    "    normal_features = list(comb_B) + [\"elapsed_day\", \"OV_lag1\", \"OV_diff\", \"OV_roll_mean3\", \"OV_roll_std3\", \"OV\"]\n",
    "    train_df = feature_engineering2(nomaly_train_df, normal_features)\n",
    "    test_df = feature_engineering2(normaly_test_df, normal_features)\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        #(\"scaler\", StandardScaler()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LGBMRegressor(\n",
    "            objective=\"regression\",\n",
    "            metric=\"rmse\",\n",
    "            verbose=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "    train_X = train_df[normal_features].drop(columns=[\"OV\"])\n",
    "    pipeline.fit(train_X, train_df[\"OV\"])\n",
    "\n",
    "    test_X = test_df[normal_features].drop(columns=[\"OV\"])\n",
    "    normal_pred = pipeline.predict(test_X)\n",
    "    rmse_no = np.sqrt(mean_squared_error(normaly_test_df[\"OV\"], normal_pred))\n",
    "    normal_pred_series = pd.Series(normal_pred, index=normaly_test_df.index)\n",
    "    df_eval = pd.concat([test_df_original[\"OV\"].rename(\"y_true\"), spiked_pred_series.rename(\"y_pred\")], axis=1)\n",
    "    df_eval[\"y_pred\"] = df_eval[\"y_pred\"].fillna((normal_pred_series))\n",
    "    rmse_all = np.sqrt(mean_squared_error(df_eval[\"y_true\"], df_eval[\"y_pred\"]))\n",
    "    \n",
    "    print(f\"COMB:{comb_A, comb_B} \\t RMSE (ALL): {rmse_all:.3f} \\t RMSE (SP): {rmse_sp:.3f} \\t RMSE (NO): {rmse_no:.3f} \\t RMSE BEST: {best_rmse:.3f}\")\n",
    "    if rmse_all < best_rmse:\n",
    "        print(\"BEST MODEL FOUND\")\n",
    "        best_rmse = rmse_all\n",
    "        best_comb = (comb_A, comb_B)\n",
    "print(f\"best_comb: {best_comb} \\t RMSE (ALL): {best_rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65547cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
